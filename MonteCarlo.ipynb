{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f012215",
   "metadata": {},
   "outputs": [
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 101\u001b[0m\n\u001b[0;32m     97\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# 다음 상태로 이동\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# 보상은 숫자이고, 완료 여부는 boolean\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m agent\u001b[38;5;241m.\u001b[39msave_sample(next_state, reward, done)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# 다음 행동 받아옴\u001b[39;00m\n",
      "File \u001b[1;32m~\\minigame\\reinforcement-learning-kr-master\\1-grid-world\\3-monte-carlo\\environment.py:101\u001b[0m, in \u001b[0;36mEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m--> 101\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrectangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     base_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\minigame\\lib\\tkinter\\__init__.py:2766\u001b[0m, in \u001b[0;36mCanvas.coords\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2762\u001b[0m \u001b[38;5;124;03m\"\"\"Return a list of coordinates for the item given in ARGS.\"\"\"\u001b[39;00m\n\u001b[0;32m   2763\u001b[0m \u001b[38;5;66;03m# XXX Should use _flatten on args\u001b[39;00m\n\u001b[0;32m   2764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk\u001b[38;5;241m.\u001b[39mgetdouble(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m   2765\u001b[0m                    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk\u001b[38;5;241m.\u001b[39msplitlist(\n\u001b[1;32m-> 2766\u001b[0m            \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)]\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from environment import Env\n",
    "\n",
    "\n",
    "# 몬테카를로 에이전트 (모든 에피소드 각각의 샘플로 부터 학습)\n",
    "class MCAgent:\n",
    "    def __init__(self, actions):\n",
    "        self.width = 5\n",
    "        self.height = 5\n",
    "        self.actions = actions\n",
    "        self.learning_rate = 0.01\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 1\n",
    "        self.samples = []\n",
    "        self.value_table = defaultdict(float)\n",
    "\n",
    "    # 메모리에 샘플을 추가\n",
    "    def save_sample(self, state, reward, done):\n",
    "        self.samples.append([state, reward, done])\n",
    "\n",
    "    # 모든 에피소드에서 에이전트가 방문한 상태의 큐 함수를 업데이트\n",
    "    def update(self):\n",
    "        G_t = 0\n",
    "        visit_state = []\n",
    "        for reward in reversed(self.samples):\n",
    "            state = str(reward[0])\n",
    "            if state not in visit_state:\n",
    "                visit_state.append(state)\n",
    "                G_t = reward[1] + self.discount_factor * G_t\n",
    "                value = self.value_table[state]\n",
    "                self.value_table[state] = (value +\n",
    "                                           self.learning_rate * (G_t - value))\n",
    "\n",
    "    # 큐 함수에 따라서 행동을 반환\n",
    "    # 입실론 탐욕 정책에 따라서 행동을 반환\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # 랜덤 행동\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # 큐 함수에 따른 행동\n",
    "            next_state = self.possible_next_state(state)\n",
    "            action = self.arg_max(next_state)\n",
    "        return int(action)\n",
    "\n",
    "    # 후보가 여럿이면 arg_max를 계산하고 무작위로 하나를 반환\n",
    "    @staticmethod\n",
    "    def arg_max(next_state):\n",
    "        max_index_list = []\n",
    "        max_value = next_state[0]\n",
    "        for index, value in enumerate(next_state):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)\n",
    "\n",
    "    # 가능한 다음 모든 상태들을 반환\n",
    "    def possible_next_state(self, state):\n",
    "        col, row = state\n",
    "        next_state = [0.0] * 4\n",
    "\n",
    "        if row != 0:\n",
    "            next_state[0] = self.value_table[str([col, row - 1])]\n",
    "        else:\n",
    "            next_state[0] = self.value_table[str(state)]\n",
    "        if row != self.height - 1:\n",
    "            next_state[1] = self.value_table[str([col, row + 1])]\n",
    "        else:\n",
    "            next_state[1] = self.value_table[str(state)]\n",
    "        if col != 0:\n",
    "            next_state[2] = self.value_table[str([col - 1, row])]\n",
    "        else:\n",
    "            next_state[2] = self.value_table[str(state)]\n",
    "        if col != self.width - 1:\n",
    "            next_state[3] = self.value_table[str([col + 1, row])]\n",
    "        else:\n",
    "            next_state[3] = self.value_table[str(state)]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "\n",
    "# 메인 함수\n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = MCAgent(actions=list(range(env.n_actions)))\n",
    "\n",
    "    for episode in range(1000):\n",
    "        state = env.reset()\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        while True:\n",
    "            env.render()\n",
    "\n",
    "            # 다음 상태로 이동\n",
    "            # 보상은 숫자이고, 완료 여부는 boolean\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.save_sample(next_state, reward, done)\n",
    "\n",
    "            # 다음 행동 받아옴\n",
    "            action = agent.get_action(next_state)\n",
    "\n",
    "            # 에피소드가 완료됐을 때, 큐 함수 업데이트\n",
    "            if done:\n",
    "                agent.update()\n",
    "                agent.samples.clear()\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0602a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
